*************************install flume*********************************
rm -rf * (deletes everything)

wget http://archive.apache.org/dist/flume/1.4.0/apache-flume-1.4.0-bin.tar.gz

tar -zxf apache-flume-1.4.0-bin.tar.gz

cd apache-flume-1.4.0-bin/conf/

ls

mv flume-env.sh.template flume-env.sh

nano /home/ubuntu/apache-flume-1.4.0-bin/conf/flume-env.sh

JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
FLUME_CLASSPATH="/home/ubuntu/apache-flume-1.4.0-bin/lib/*.jar"

cd

jps

nano apache-flume-1.4.0-bin/conf/flume.conf

************************Source/Channel/Sinks setup*********************
# Flume agent config
cloudage.sources = eventlog
cloudage.channels = file_channel
cloudage.sinks = sink_to_hdfs

# Define / Configure source
cloudage.sources.eventlog.type = exec
cloudage.sources.eventlog.command = tail -F /var/log/flume/eventlog.log
cloudage.sources.eventlog.restart = true
cloudage.sources.eventlog.batchSize = 1000
#cloudage.sources.eventlog.type = seq

# HDFS sinks
cloudage.sinks.sink_to_hdfs.type = hdfs
cloudage.sinks.sink_to_hdfs.hdfs.fileType = DataStream
cloudage.sinks.sink_to_hdfs.hdfs.path = hdfs://localhost:9000/user/ubuntu/flume/events
cloudage.sinks.sink_to_hdfs.hdfs.filePrefix = eventlog
cloudage.sinks.sink_to_hdfs.hdfs.fileSuffix = .log
cloudage.sinks.sink_to_hdfs.hdfs.batchSize = 1000

# Use a channel which buffers events in memory
cloudage.channels.file_channel.type = file
cloudage.channels.file_channel.checkpointDir = /var/log/flume/checkpoint
cloudage.channels.file_channel.dataDirs = /var/log/flume/data

# Bind the source and sink to the channel
cloudage.sources.eventlog.channels = file_channel
cloudage.sinks.sink_to_hdfs.channel = file_channel

************************************************************************
wget https://s3-ap-southeast-2.amazonaws.com/datasetz/generate_logs.py

sudo nano generate_logs.py

#rename logfile default to /var/log/flume/eventlog.log (edit DEFAULTS)#
	
ll /var/

sudo mkdir /var/log/flume/
sudo mkdir /var/log/flume/checkpoint/
sudo mkdir /var/log/flume/data/
sudo chmod 777 -R /var/log/flume

sudo python generate_logs.py

**************************open local terminal**************************
#login to your instance again#

tail -F /var/log/flume/eventlog.log

#go back to your old instance#
nano apache-flume-1.4.0-bin/conf/flume.conf

hadoop fs -mkdir hdfs://localhost:9000/user/ubuntu/flume/events

#go back to new instance#
cd apache-flume-1.4.0-bin/bin

./flume-ng agent --conf /home/ubuntu/apache-flume-1.4.0-bin/conf/ --conf-file /home/ubuntu/apache-flume-1.4.0-bin/conf/flume.conf  --name cloudage

#go back to old instace#
sudo python generate_logs.py


#refresh your cluster's event folder#

******************login to twitter for live feed***********************

apps.twitter.com

Create an application

click on keys and access tokens

generate app setting key

generate access token key

copy API Key/API Secret & Token key/Token Secret

wget https://s3-ap-southeast-2.amazonaws.com/datasetz/flume-sources-1.0-SNAPSHOT.jar

nano apache-flume-1.4.0-bin/conf/flume-env.sh
#ignore comment FLUME_CLASSPATH...

FLUME_CLASSPATH="apache-flume-1.4.0-bin/lib/flume-sources-1.0-SNAPSHOT.jar"

mv flume-sources-1.0-SNAPSHOT.jar apache-flume-1.4.0-bin/lib/

nano twitter.conf
*************************configuiring Twitter file*********************
TwitterAgent.sources = Twitter
TwitterAgent.channels = MemChannel
TwitterAgent.sinks = HDFS

TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSource
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sources.Twitter.consumerKey = (CONSUMER KEY)
TwitterAgent.sources.Twitter.consumerSecret = (CONSUMER SECRET)
TwitterAgent.sources.Twitter.accessToken = (ACCESS TOKEN KEY)
TwitterAgent.sources.Twitter.accessTokenSecret = (ACCESS TOKEN SECRET)

TwitterAgent.sources.Twitter.keywords = hadoop, big data, analytics, bigdata, cloudera, data science, data scientist, business intelligence, mapreduce

TwitterAgent.sinks.HDFS.channel = MemChannel
TwitterAgent.sinks.HDFS.type = hdfs
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://localhost:9000/user/ubuntu/tweeter
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text
TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
TwitterAgent.sinks.HDFS.hdfs.rollInterval = 900

TwitterAgent.channels.MemChannel.type = memory
TwitterAgent.channels.MemChannel.capacity = 10000
TwitterAgent.channels.MemChannel.transactionCapacity = 10000

************************************************************************
mv twitter.conf apache-flume-1.4.0-bin/conf/

cd /home/ubuntu/apache-flume-1.4.0-bin/bin

./flume-ng agent --conf /home/ubuntu/apache-flume-1.4.0-bin/conf/  -f /home/ubuntu/apache-flume-1.4.0-bin/conf/twitter.conf -Dflume.root.logger=DEBUG,console -n TwitterAgent

#go to PublicIP:50070 and check the live feeds /user/unbuntu/twitter#

#minimise your terminal as it will take 10-15 min for it to get feeds#
****************************** mysql **********************************

sudo apt-get install mysql-server mysql-client -y

#set your password#

sudo service mysql status

mysql -u root -p
*********************creating a database in mysql**********************
show databases;
CREATE DATABASE kadak_db;
GRANT ALL PRIVILEGES ON kadak_db.* TO 'root'@'localhost';
show databases;
USE kadak_db;
show tables;

CREATE TABLE user_data(first_name VARCHAR(50) NOT NULL,
  company_name VARCHAR(100),
  address VARCHAR(100),
 country VARCHAR(50),
 city VARCHAR(50),
 state VARCHAR(50));

show tables;
desc user_data;
 
exit;

***************get the file and upload it in tables********************

wget https://s3-ap-southeast-2.amazonaws.com/datasetz/userdata.txt

sudo cp userdata.txt /var/lib/mysql/kadak_db/

mysql -u root -p
USE kadak_db;
SHOW VARIABLES LIKE 'secure_file_priv'; 
exit ;
 
sudo nano /etc/mysql/my.cnf
#go to basic settings and paste the file privilage below, blank value#
secure_file_priv = ""

sudo service mysql restart

mysql -u root -p
USE kadak_db;
LOAD DATA INFILE 'userdata.txt' INTO TABLE user_data FIELDS TERMINATED BY ',' ;
SELECT COUNT(*) FROM user_data;
select * from user_data limit 10;
quit;

****************experimenting with 10000 rows sql file*****************

wget https://s3-ap-southeast-2.amazonaws.com/datasetz/Sample-SQL-File-10000-Rows.sql

mysql -u root -p kadak_db < Sample-SQL-File-10000-Rows.sql

mysql -u root -p 

show databases;
use kadak_db;
desc user_details;
select * from user_details limit 100;
exit;

#for backup#
mysqldump -u root -p --all-databases; > mysql_15-10-2016.sql

#for restore#
mysql -u root -p < mysql_15-10-2016.sql

********************************Sqoop**********************************

wget http://archive.apache.org/dist/sqoop/1.4.4/sqoop-1.4.4.bin__hadoop-1.0.0.tar.gz

tar -zxf sqoop-1.4.4.bin__hadoop-1.0.0.tar.gz

sudo mv sqoop-1.4.4.bin__hadoop-1.0.0 /usr/local/sqoop/

nano ~/.bashrc

export SQOOP_PREFIX="/usr/local/sqoop/"
export SQOOP_CONF_DIR="$SQOOP_PREFIX/conf"
export SQOOP_CLASSPATH="$SQOOP_CONF_DIR"
export PATH="$SQOOP_PREFIX/bin:$PATH"

bash

cd $SQOOP_PREFIX/conf

mv sqoop-env-template.sh sqoop-env.sh

nano sqoop-env.sh

export HADOOP_COMMON_HOME=/usr/local/hadoop 
export HADOOP_MAPRED_HOME=/usr/local/hadoop
export ZOOKEPER_HOME=$ZOOKEEPER_HOME
export HBASE_HOME=$HBASE_HOME
export HIVE_HOME=$HIVE_HOME

cd

wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.39.tar.gz

tar -zxf mysql-connector-java-5.1.39.tar.gz

cp mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar /usr/local/sqoop/lib/

cd $SQOOP_PREFIX/bin

sqoop-version

sqoop import --connect jdbc:mysql://localhost/kadak_db --table user_details -m 1 --target-dir /tables/userdata/

sqoop import --connect jdbc:mysql://localhost/kadak_db --username root -P --table user_data -m 1 --target-dir /tables/userdata


#check your sqoop data in GUI (tables/userdata/part-m-00000)#
#check twitter logs in GUI#
#go to hue browser or JSON validator to validate the Twitter data#
# experiment on twitterfiles or with tables and keep practicing#
******************************THE END**********************************
